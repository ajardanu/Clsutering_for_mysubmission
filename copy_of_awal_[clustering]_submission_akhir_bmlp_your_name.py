# -*- coding: utf-8 -*-
"""Copy of  awal [Clustering] Submission Akhir BMLP_Your Name.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qVq4pBW66awnJrFQsmCzp_wH8GUln9-Z

# **Penting**
- Jangan mengubah atau menambahkan cell text yang sudah disediakan, Anda hanya perlu mengerjakan cell code yang sudah disediakan.
- Pastikan seluruh kriteria memiliki output yang sesuai, karena jika tidak ada output dianggap tidak selesai.
- Misal, Anda menggunakan df = df.dropna() silakan gunakan df.isnull().sum() sebagai tanda sudah berhasil. Silakan sesuaikan seluruh output dengan perintah yang sudah disediakan.
- Pastikan Anda melakukan Run All sebelum mengirimkan submission untuk memastikan seluruh cell berjalan dengan baik.
- Pastikan Anda menggunakan variabel df dari awal sampai akhir dan tidak diperbolehkan mengganti nama variabel tersebut.
- Hapus simbol pagar (#) pada kode yang bertipe komentar jika Anda menerapkan kriteria tambahan
- Biarkan simbol pagar (#) jika Anda tidak menerapkan kriteria tambahan
- Pastikan Anda mengerjakan sesuai section yang sudah diberikan tanpa mengubah judul atau header yang disediakan.

# **INFORMASI DATASET**

Dataset ini menyajikan gambaran mendalam mengenai perilaku transaksi dan pola aktivitas keuangan, sehingga sangat ideal untuk eksplorasi **deteksi penipuan (fraud detection)** dan **identifikasi anomali**. Dataset ini mencakup **2.512 sampel data transaksi**, yang mencakup berbagai atribut transaksi, demografi nasabah, dan pola penggunaan.

Setiap entri memberikan wawasan komprehensif terhadap perilaku transaksi, memungkinkan analisis untuk **keamanan finansial** dan pengembangan model prediktif.

## Fitur Utama

- **`TransactionID`**: Pengidentifikasi unik alfanumerik untuk setiap transaksi.  
- **`AccountID`**: ID unik untuk setiap akun, dapat memiliki banyak transaksi.  
- **`TransactionAmount`**: Nilai transaksi dalam mata uang, mulai dari pengeluaran kecil hingga pembelian besar.  
- **`TransactionDate`**: Tanggal dan waktu transaksi terjadi.  
- **`TransactionType`**: Tipe transaksi berupa `'Credit'` atau `'Debit'`.  
- **`Location`**: Lokasi geografis transaksi (nama kota di Amerika Serikat).  
- **`DeviceID`**: ID perangkat yang digunakan dalam transaksi.  
- **`IP Address`**: Alamat IPv4 yang digunakan saat transaksi, dapat berubah untuk beberapa akun.  
- **`MerchantID`**: ID unik merchant, menunjukkan merchant utama dan anomali transaksi.  
- **`AccountBalance`**: Saldo akun setelah transaksi berlangsung.  
- **`PreviousTransactionDate`**: Tanggal transaksi terakhir pada akun, berguna untuk menghitung frekuensi transaksi.  
- **`Channel`**: Kanal transaksi seperti `Online`, `ATM`, atau `Branch`.  
- **`CustomerAge`**: Usia pemilik akun.  
- **`CustomerOccupation`**: Profesi pengguna seperti `Dokter`, `Insinyur`, `Mahasiswa`, atau `Pensiunan`.  
- **`TransactionDuration`**: Lama waktu transaksi (dalam detik).  
- **`LoginAttempts`**: Jumlah upaya login sebelum transaksiâ€”jumlah tinggi bisa mengindikasikan anomali.

Tugas kamu adalah membuat model clustering yang selanjutnya akan digunakan untuk membuat model klasifikasi.

# **1. Import Library**
Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning. Semua library yang dibutuhkan harus **import** di **cell** ini, jika ada library yang dijalankan di cell lain maka **submission langsung ditolak**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
from sklearn.metrics import silhouette_score

"""# **2. Memuat Dataset**
Pada tahap ini, Anda perlu memuat dataset ke dalam notebook lalu mengecek informasi dataset sebelum nantinya dilakukan pembersihan. Hal-hal yang perlu dilakukan pada tahapan ini yaitu:
1. **Memahami Struktur Data**
   - Dataset harus mengambil referensi wajib digunakan (bisa dilihat [Disini](https://drive.google.com/drive/folders/1Zs7VmPZ-jNwsRlMKH65Ea-LApSwx6lKx?usp=drive_link))
   - Melakukan loading dataset ke dalam notebook dan menampilkan 5 baris pertama dengan function `head`.
   - Tinjau jumlah baris kolom dan jenis data dalam dataset dengan function `info`.  
   - Menampilkan statistik deskriptif dataset dengan menjalankan `describe`.
   - Pastikan **setiap function tersebut** memiliki **output pada setiap cell** code. Jika tidak **submission langsung ditolak**

Gunakan code ini untuk melakukan load data secara otomatis tanpa harus download data tersebut secara manual:
```python
url='https://drive.google.com/uc?id=1gnLO9qvEPqv1uBt1928AcsCmdvzqjC5m'
df = pd.read_csv(url)
```

Penting: pada kriteria pertama hindari penggunaan print() dan display() karena seluruh fungsi yang digunakan sudah memiliki standar output dan menghasilkan output yang diharapkan.

Kriteria 1 akan ditolak ketika:
- print(__.head())
- display(___.head())
dst

Kriteria 1 akan diterima ketika Anda menggunakan fungsi yang diminta tanpa menambahkan deskripsi apapun.
"""

url='https://drive.google.com/uc?id=1gnLO9qvEPqv1uBt1928AcsCmdvzqjC5m'
df = pd.read_csv(url)

df.head()

df.info()

df.describe()

"""(Opsional) Memuat Dataset dan Melakukan Exploratory Data Analysis (EDA) [Skilled]

**Biarkan kosong jika tidak menerapkan kriteria skilled**

**Apabila ingin menerapkan Advanced, pastikan seluruh visualisasi tidak ada yang overlap**
"""

correlation_matrix = df.corr(numeric_only=True)

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, square=True)
plt.title('Correlation Matrix', fontsize=16)
plt.show()

num_vars = df.shape[1]

n_cols = 4
n_rows = -(-num_vars // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))

axes = axes.flatten()

for i, column in enumerate(df.columns):
    df[column].hist(ax=axes[i], bins=20, edgecolor='black')
    axes[i].set_title(column)
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Frequency')

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""(Opsional) Memuat Dataset dan Melakukan Exploratory Data Analysis (EDA) [Advanced]

**Biarkan kosong jika tidak menerapkan kriteria advanced**
"""

# Visualisasi yang lebih informatif (Opsional Advanced 1)

"""# **3. Pembersihan dan Pra Pemrosesan Data**

Pada tahap ini, Anda akan melakukan **Pembersihan Dataset** untuk menjadikan dataset mudah diintepretasi dan bisa dilatih. Hal-hal yang wajib kamu lakukan yaitu:

1. **Mengecek dataset** menggunakan isnull().sum() dan duplicated().sum().
2. Melakukan feature scaling menggunakan `MinMaxScaler()` atau `StandardScalar()` untuk fitur numerik.
3. Melakukan feature encoding menggunakan `LabelEncoder()` untuk fitur kategorikal.
4. Melakukan drop pada kolom id.
5. **Ketentuan Cell Code**
   - Pastikan **setiap pemeriksaan tersebut** memiliki **output pada cell-nya**. Jika tidak **submission langsung ditolak**

"""

df.isnull().sum()

df.duplicated().sum()

numeric_features = df.select_dtypes(include=['number']).columns

scaler = StandardScaler()
df[numeric_features] = scaler.fit_transform(df[numeric_features])
df.head()

FiturTidakRelevan = ['TransactionID', 'AccountID','DeviceID', 'IP Address', 'MerchantID']

df = df.drop(columns=FiturTidakRelevan, errors='ignore')
df.head()

categorical_features = df.select_dtypes(include=['object']).columns
label_encoder = LabelEncoder()
df_lencoder = pd.DataFrame(df)

for col in categorical_features:
    df_lencoder[col] = label_encoder.fit_transform(df[col])

df_lencoder.head()

df_lencoder.columns.tolist()

"""(Opsional) Pembersihan dan Pra Pemrosesan Data [Skilled]

**Biarkan kosong jika tidak menerapkan kriteria skilled**
"""

for col in numeric_features:
    df_lencoder[col] = df_lencoder[col].fillna(df_lencoder[col].mean())

for col in categorical_features:
    df_lencoder[col] = df_lencoder[col].fillna(df_lencoder[col].mode()[0])
df_lencoder.isnull().sum()

duplicates = df_lencoder.duplicated()
df_lencoder = df_lencoder.drop_duplicates()
df_lencoder.head()

"""(Opsional) Pembersihan dan Pra Pemrosesan Data [Advanced]

**Biarkan kosong jika tidak menerapkan kriteria advanced**
"""

Q1 = df_lencoder[numeric_features].quantile(0.25)
Q3 = df_lencoder[numeric_features].quantile(0.75)
IQR = Q3 - Q1

condition = ~((df_lencoder[numeric_features] < (Q1 - 1.5 * IQR)) |
              (df_lencoder[numeric_features] > (Q3 + 1.5 * IQR))).any(axis=1)

df_lencoder = df_lencoder.loc[condition].reset_index(drop=True)

for feature in numeric_features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df_lencoder[feature])
    plt.title(f'Box Plot of {feature}')
    plt.show()

# Melakukan binning data berdasarkan kondisi rentang nilai pada fitur numerik,
# lakukan pada satu sampai dua fitur numerik.
# Silahkan lakukan encode hasil binning tersebut menggunakan LabelEncoder.
# Pastikan kamu mengerjakan tahapan ini pada satu cell.

"""# **4. Membangun Model Clustering**
Pada tahap ini, Anda membangun model clustering dengan memilih algoritma yang sesuai untuk mengelompokkan data berdasarkan kesamaan.
1. Pastikan Anda menggunakan dataframe yang sudah melalui processing sesuai dengan levelnya (Basic, Skilled, Advanced)
2. Melakukan visualisasi Elbow Method untuk menentukan jumlah cluster terbaik menggunakan `KElbowVisualizer()`.
3. Menggunakan algoritma K-Means Clustering dengan `sklearn.cluster.KMeans()`.
4. Jalankan cell code `joblib.dump(model_kmeans, "model_clustering.h5")` untuk menyimpan model yang sudah dibuat.
"""

df_lencoder.describe()

kmeans = KMeans()

visualizer = KElbowVisualizer(kmeans, k=(1, 10))

visualizer.fit(df_lencoder)

visualizer.show()

k = 3
kmeans = KMeans(n_clusters=k, random_state=0)
labels = kmeans.fit_predict(df_lencoder)

"""Jalankan cell code ini untuk menyimpan model kamu."""

joblib.dump(kmeans, "model_clustering.h5")

"""(Opsional) Membangun Model Clustering [Skilled]

**Biarkan kosong jika tidak menerapkan kriteria skilled**
"""

sil_score = silhouette_score(df_lencoder, labels)

print(f"Silhouette Score untuk k={k}: {sil_score:.4f}")

pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_lencoder)

plt.figure(figsize=(8,6))
plt.scatter(df_pca[:,0], df_pca[:,1], c=labels, cmap="viridis", alpha=0.6)
plt.title("Visualisasi Cluster dengan PCA (k=3)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label="Cluster")
plt.show()

"""(Opsional) Membangun Model Clustering [Advanced]

**Biarkan kosong jika tidak menerapkan kriteria advanced**
"""

pca = PCA(n_components=2)

df_pca = pca.fit_transform(df_lencoder)

data_final = pd.DataFrame(df_pca, columns=['PCA1', 'PCA2'])

kmeans = KMeans(n_clusters=k, random_state=0)
kmeans.fit(data_final)

labels = kmeans.labels_

data_final['target'] = labels
score = silhouette_score(data_final.drop("target", axis=1), labels)

print(f"Silhouette Score untuk k={k}: {score:.4f}")

joblib.dump(kmeans, "PCA_model_clustering.h5")

"""# **5. Interpretasi Cluster**

## **a. Interpretasi Hasil Clustering**
1. **Contoh Interpretasi:**
- **Cluster 1: (Nasabah Bertransaksi dan Pendapatan Besar)**:
  - **Rata-rata (mean) Annual Income:** 0.953 (48,260)
  - **Rata-rata (mean) Spending Score:** 0.8 (56.48)
  - **Analisis:** Cluster ini mencakup pelanggan dengan pendapatan tahunan tinggi dan tingkat pengeluaran yang cukup tinggi. Pelanggan dalam cluster ini cenderung memiliki daya beli yang tinggi dan mereka lebih cenderung untuk membelanjakan sebagian besar pendapatan mereka. Sehingga rekomendasi pada kelompok nasabah ini adalah dengan menawarkan produk-produk investasi atau perbankan yang berkualitas tinggi.
"""

df_clustered = df_lencoder.copy()
df_clustered["target"] = data_final["target"]

fitur_pilihan = ["TransactionAmount", "CustomerAge",
                 "TransactionDuration", "LoginAttempts",
                 "AccountBalance", "target"]

df_pilihan = df_clustered[fitur_pilihan]

deskriptif = df_pilihan.groupby("target").agg(["mean", "min", "max"])
deskriptif

"""## Menjelaskan karakteristik tiap cluster berdasarkan rentangnya.
1. **Cluster 1: (Nasabah Stabil)**:
  - **Rata-rata (mean) <Fitur> jumlah transaksi: 0.1131** <Sebelum inverse> <Setelah inverse>
  - **Rata-rata (mean) <Fitur> saldo rekening: -0.0232** <Sebelum inverse> <Setelah inverse>
  - **Analisis:** Cluster ini mencakup nasabah dengan usia sedikit lebih tinggi dari rata-rata dan tingkat transaksi yang cukup seimbang. Saldo akun mereka stabil (mendekati rata-rata), dan durasi transaksi cenderung normal. Mereka bisa dikategorikan sebagai nasabah stabil yang sudah memiliki hubungan jangka menengah dengan bank tawarkan produk tabungan dengan bunga menarik, serta paket layanan loyalitas agar hubungan tetap terjaga.
2. **Cluster 2: (Nasabah Aktif Muda)**:
  - **Rata-rata (mean) <Fitur> jumlah transaksi: -0.1590** <Sebelum inverse> <Setelah inverse>
  - **Rata-rata (mean) <Fitur> saldo rekening: 0.0359** <Sebelum inverse> <Setelah inverse>
  - **Analisis:** Cluster ini berisi nasabah yang cenderung lebih muda, dengan aktivitas transaksi sedikit lebih tinggi (ditunjukkan durasi dan saldo positif dibanding cluster lain). Walaupun nilai transaksi rata-rata lebih rendah, mereka memiliki saldo akun positif, tanda mereka mulai aktif menggunakan layanan perbankan.berikan promosi digital banking, cashback, atau produk investasi ringan agar mereka semakin aktif.
3. **Cluster 3: (Nasabah Baru atau pasif)**:
  - **Rata-rata (mean) <Fitur> jumlah transaksi: -0.1597** <Sebelum inverse> <Setelah inverse>
  - **Rata-rata (mean) <Fitur> saldo rekening: -0.0092** <Sebelum inverse> <Setelah inverse>
  - **Analisis:** Cluster ini mencerminkan nasabah dengan usia relatif lebih muda namun aktivitas transaksi dan saldo lebih rendah dari cluster lain. Mereka bisa dikategorikan sebagai nasabah pasif, yang mungkin hanya menggunakan rekening untuk kebutuhan dasar (menabung atau menerima gaji).lakukan edukasi finansial, tawarkan paket layanan entry-level, dan beri insentif agar mereka lebih sering bertransaksi.

# **6. Mengeksport Data**

1. Simpan nama kolom hasil clustering dengan nama `Target`.
2. Simpan hasilnya ke dalam file CSV menggunakan function `to_csv()`.
"""

df_clustered.head()

df_clustered.to_csv('data_clustering.csv', index=False)

"""(Opsional) Interpretasi Hasil Clustering [Skilled]

**Biarkan kosong jika tidak menerapkan kriteria skilled**
"""

df_clustered[numeric_features] = scaler.inverse_transform(df_clustered[numeric_features])
df_clustered.head()

# inverse dataset yang sudah diencode ke kategori aslinya.
# Lengkapi kode berikut jika ingin menerapkan kriteria ini (silakan hapus simbol pagar pada kode yang akan digunakan.)
# for ___ in categorical_cols:
#     ___ = encoders[col]
#     df[col] = ___.inverse_transform(df_inverse[col].astype(int))
# tampilkan dataset yang sudah di-inverse
# ___.head()

# Lakukan analisis deskriptif minimal mean, min dan max untuk fitur numerik dan mode untuk kategorikal seperti pada basic tetapi menggunakan data yang sudah diinverse.
# pastikan output menghasilkan agregasi dan groupby bersamaan dengan mean, min, dan max kembali setelah melakukan inverse.

"""## Menjelaskan karakteristik tiap cluster berdasarkan rentangnya setelah inverse.
1. **Cluster 1: (___)**:
  - **Rata-rata (mean) <Fitur>:** <Sebelum inverse> <Setelah inverse>
  - **Analisis:** Cluster ini ...

(Opsional) Interpretasi Hasil Clustering [Advanced]

**Biarkan kosong jika tidak menerapkan kriteria advanced**
"""

# Mengintegrasikan kembali data yang telah di-inverse dengan hasil cluster.

# Simpan Data
# ___.to_csv('data_clustering_inverse.csv', index=False)

"""End of Code."""